{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oolong Benchmark for RLM-ADK\n",
    "\n",
    "Evaluate long-context aggregation reasoning using the [Oolong benchmark](https://arxiv.org/abs/2511.02817).\n",
    "\n",
    "This notebook runs RLM with `grok-4-1-fast-reasoning` on the Oolong datasets,\n",
    "which test classification, counting, and comparison across large volumes of text.\n",
    "No frontier model exceeds 50% accuracy at 128K context -- RLM's recursive\n",
    "decomposition approach aims to beat that ceiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies and clone repo\n",
    "!pip install xai-sdk tiktoken python-dotenv datasets -q\n",
    "!git clone https://github.com/fabiopauli/rlm-adk.git 2>/dev/null || (cd rlm-adk && git pull)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'rlm-adk')\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load API key from Colab secrets\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "os.environ['XAI_API_KEY'] = userdata.get('XAI_API_KEY')\n",
    "print('API key loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "# Choose a profile: \"smoke\", \"quick\", \"moderate\" (default), \"standard_validation\", or \"custom\"\n",
    "PROFILE = \"moderate\"\n",
    "\n",
    "PROFILES = {\n",
    "    \"smoke\":                {\"subset\": 5,    \"max_cost\": 1.0,   \"max_cost_per_q\": 0.50},\n",
    "    \"quick\":                {\"subset\": 25,   \"max_cost\": 5.0,   \"max_cost_per_q\": 0.50},\n",
    "    \"moderate\":             {\"subset\": 50,   \"max_cost\": 5.0,   \"max_cost_per_q\": 0.50},\n",
    "    \"standard_validation\":  {\"subset\": None, \"max_cost\": 100.0, \"max_cost_per_q\": 0.50},\n",
    "}\n",
    "\n",
    "# Custom overrides (only used if PROFILE == \"custom\")\n",
    "CUSTOM_SUBSET = 10\n",
    "CUSTOM_MAX_COST = 2.0\n",
    "CUSTOM_MAX_COST_PER_Q = 0.50\n",
    "\n",
    "if PROFILE == \"custom\":\n",
    "    cfg = {\"subset\": CUSTOM_SUBSET, \"max_cost\": CUSTOM_MAX_COST, \"max_cost_per_q\": CUSTOM_MAX_COST_PER_Q}\n",
    "else:\n",
    "    cfg = PROFILES[PROFILE]\n",
    "\n",
    "# Model configuration\n",
    "MODEL = \"grok-4-1-fast-reasoning\"\n",
    "PROVIDER = \"xai\"\n",
    "DATASET = \"synth\"       # \"synth\" or \"real\"\n",
    "SPLIT = \"validation\"    # \"validation\" or \"test\"\n",
    "REAL_CONFIG = \"toy_dnd\"  # \"toy_dnd\" or \"dnd\" (for real dataset)\n",
    "\n",
    "print(f\"Profile: {PROFILE}\")\n",
    "print(f\"  Subset: {cfg['subset'] or 'all'} questions\")\n",
    "print(f\"  Max cost: ${cfg['max_cost']:.2f}\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Dataset: {DATASET} ({SPLIT})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load data\n",
    "from tests.test_oolong import OolongBenchmark\n",
    "\n",
    "bench = OolongBenchmark(\n",
    "    api_key=os.environ['XAI_API_KEY'],\n",
    "    model=MODEL,\n",
    "    provider=PROVIDER,\n",
    "    verbose=True,\n",
    "    max_cost_per_question=cfg['max_cost_per_q'],\n",
    "    max_cost_total=cfg['max_cost'],\n",
    ")\n",
    "\n",
    "if DATASET == 'synth':\n",
    "    windows = bench.load_synth(split=SPLIT, max_samples=cfg['subset'])\n",
    "else:\n",
    "    windows = bench.load_real(config=REAL_CONFIG, split=SPLIT, max_samples=cfg['subset'])\n",
    "\n",
    "total_q = sum(len(qs) for qs in windows.values())\n",
    "print(f\"\\nLoaded {total_q} questions across {len(windows)} context windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run benchmark\n",
    "if DATASET == 'synth':\n",
    "    summary = bench.run_synth(\n",
    "        split=SPLIT,\n",
    "        max_samples=cfg['subset'],\n",
    "    )\n",
    "else:\n",
    "    summary = bench.run_real(\n",
    "        config=REAL_CONFIG,\n",
    "        split=SPLIT,\n",
    "        max_samples=cfg['subset'],\n",
    "    )\n",
    "\n",
    "bench.print_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Results summary\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "s = summary.to_dict()\n",
    "print(f\"\\nOverall Score: {s['overall_score']:.3f}\")\n",
    "print(f\"Questions: {s['num_questions']} (scored: {s['num_scored']}, errors: {s['num_errors']})\")\n",
    "print(f\"Total Cost: ${s['total_cost']:.2f}\")\n",
    "print(f\"Total Tokens: {s['total_tokens']:,}\")\n",
    "\n",
    "if s['score_by_answer_type']:\n",
    "    print(\"\\nBy Answer Type:\")\n",
    "    for k, v in sorted(s['score_by_answer_type'].items()):\n",
    "        print(f\"  {k:20s}: {v:.3f}\")\n",
    "\n",
    "if s['score_by_task']:\n",
    "    print(\"\\nBy Task:\")\n",
    "    for k, v in sorted(s['score_by_task'].items()):\n",
    "        print(f\"  {k:20s}: {v:.3f}\")\n",
    "\n",
    "if s['score_by_context_len']:\n",
    "    print(\"\\nBy Context Length:\")\n",
    "    for k, v in s['score_by_context_len'].items():\n",
    "        print(f\"  {k:20s}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save and download results\n",
    "from google.colab import files\n",
    "from datetime import datetime\n",
    "\n",
    "filename = f\"oolong_results_{DATASET}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "bench.save_results(filename)\n",
    "files.download(filename)\n",
    "print(f\"Results saved and downloading: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Debug -- Run a single question with verbose output\n# Pick the first context window and first question for inspection\nfrom tests.test_oolong import OolongScorer\nfrom rlm.core import RecursiveLanguageModel\n\ndebug_windows = bench.load_synth(split=SPLIT, max_samples=1)\ndebug_cw_id = list(debug_windows.keys())[0]\ndebug_q = debug_windows[debug_cw_id][0]\n\nprint(f\"Context window: {debug_cw_id}\")\nprint(f\"Question: {debug_q['question']}\")\nprint(f\"Answer type: {debug_q.get('answer_type', 'N/A')}\")\nprint(f\"Gold answer: {debug_q.get('answer', debug_q.get('gold_answer', 'N/A'))}\")\nprint(f\"Context length: {debug_q.get('context_len', 'N/A')} tokens\")\nprint(f\"Context preview: {debug_q.get('context_window_text', '')[:200]}...\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Running RLM (force_repl=True)...\")\nprint(\"=\" * 60 + \"\\n\")\n\ndebug_rlm = RecursiveLanguageModel(\n    model=MODEL,\n    provider=PROVIDER,\n    xai_api_key=os.environ['XAI_API_KEY'],\n    max_cost=cfg['max_cost_per_q'],\n    enable_cache=True,\n    log_level='INFO',\n)\n\ndebug_answer = debug_rlm.run(\n    task=debug_q['question'],\n    context=debug_q['context_window_text'],\n    verbose=True,\n    force_repl=True,\n)\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"Raw answer: {debug_answer}\")\ngold = debug_q.get('answer', debug_q.get('gold_answer', ''))\nanswer_type = str(debug_q.get('answer_type', '')).replace('ANSWER_TYPE.', '')\nparsed = OolongScorer.parse_synth_answer(str(debug_answer), answer_type)\nscore = OolongScorer.score_synth(gold, str(debug_answer), answer_type)\nprint(f\"Parsed answer: {parsed}\")\nprint(f\"Gold answer: {gold}\")\nprint(f\"Score: {score:.3f}\")\ndebug_rlm.print_metrics()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Standard validation run with checkpoint + Google Drive persistence\n",
    "# Mount Google Drive for persistent checkpoint storage across Colab reconnects\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/oolong_benchmarks'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'oolong_validation_checkpoint.json')\n",
    "\n",
    "print(f\"Checkpoint will be saved to: {CHECKPOINT_PATH}\")\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(\"Existing checkpoint found -- will resume from it.\")\n",
    "\n",
    "validation_bench = OolongBenchmark(\n",
    "    api_key=os.environ['XAI_API_KEY'],\n",
    "    model=MODEL,\n",
    "    provider=PROVIDER,\n",
    "    verbose=True,\n",
    "    max_cost_per_question=0.50,\n",
    "    max_cost_total=100.0,\n",
    ")\n",
    "\n",
    "validation_summary = validation_bench.run_synth(\n",
    "    split='validation',\n",
    "    max_samples=None,  # All questions\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    ")\n",
    "\n",
    "validation_bench.print_summary(validation_summary)\n",
    "\n",
    "# Save final results to Drive\n",
    "final_path = os.path.join(CHECKPOINT_DIR, f\"oolong_validation_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "validation_bench.save_results(final_path)\n",
    "print(f\"Final results saved to: {final_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}